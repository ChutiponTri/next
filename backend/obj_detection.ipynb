{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housepital Object Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def get_transform():\n",
    "    return T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Resize((800, 800)),  # Ensure consistent size\n",
    "    ])\n",
    "\n",
    "def validate_and_fix_boxes(target):\n",
    "    boxes = target[\"boxes\"]\n",
    "    \n",
    "    # Ensure boxes is 2D with shape [num_boxes, 4]\n",
    "    if boxes.dim() == 1:\n",
    "        boxes = boxes.view(-1, 4)  # Reshape to 2D if it's 1D\n",
    "    \n",
    "    # Ensure width and height are >= 1\n",
    "    x_min, y_min, x_max, y_max = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    x_max = torch.max(x_min + 1, x_max)  # Ensure width is >= 1\n",
    "    y_max = torch.max(y_min + 1, y_max)  # Ensure height is >= 1\n",
    "    \n",
    "    # Fix boxes and return them\n",
    "    target[\"boxes\"] = boxes\n",
    "    return target\n",
    "\n",
    "\n",
    "# Define a custom dataset class for COCO\n",
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, annotation_file, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.transforms = transforms\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.img_dir, image_info['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        annotations = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Prepare targets\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            bbox = ann['bbox']\n",
    "            # COCO uses [x, y, width, height], but PyTorch uses [x_min, y_min, x_max, y_max]\n",
    "            boxes.append([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
    "            labels.append(ann['category_id'])\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target = {\"boxes\": boxes, \"labels\": labels, \"image_id\": torch.tensor([image_id])}\n",
    "\n",
    "        # Apply transforms if provided\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        target = validate_and_fix_boxes(target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Paths\n",
    "train_dir = \"dataset/objectDetection/train\"\n",
    "train_annotations = \"dataset/objectDetection/train/_annotations.coco.json\"\n",
    "valid_dir = \"dataset/objectDetection/valid\"\n",
    "valid_annotations = \"dataset/objectDetection/valid/_annotations.coco.json\"\n",
    "\n",
    "# Datasets and loaders\n",
    "batch_size = 2\n",
    "train_dataset = COCODataset(train_dir, train_annotations, transforms=get_transform())\n",
    "valid_dataset = COCODataset(valid_dir, valid_annotations, transforms=get_transform())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to get the model\n",
    "def get_model(num_classes):\n",
    "    # Load a pre-trained Faster R-CNN model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Modify the classifier to match the number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set custom directory for model downloads\n",
    "os.environ[\"TORCH_HOME\"] = \"D:\\Practice Housepital Back\\model\\pretrain\"\n",
    "\n",
    "# Hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 4  # 3 pressure ulcer stages + 1 background\n",
    "model = get_model(num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 377.9687\n",
      "Epoch [2/10], Loss: 459.7818\n",
      "Epoch [3/10], Loss: 373.9950\n",
      "Epoch [4/10], Loss: 363.8139\n",
      "Epoch [5/10], Loss: 330.3437\n",
      "Epoch [6/10], Loss: 327.3789\n",
      "Epoch [7/10], Loss: 316.0093\n",
      "Epoch [8/10], Loss: 304.4459\n",
      "Epoch [9/10], Loss: 354.5105\n",
      "Epoch [10/10], Loss: 291.0981\n",
      "Training Complete!\n",
      "Model Saved Successfully\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        targets = [validate_and_fix_boxes(target) for target in targets]\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "torch.save(model.state_dict(), \"model/v1.pth\")\n",
    "print(\"Model Saved Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 86.92%\n",
      "Recall: 86.92%\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_images = 0\n",
    "    total_correct = 0\n",
    "    total_ground_truth = 0\n",
    "    iou_threshold = 0.5  # Threshold for a \"correct\" detection\n",
    "\n",
    "    for images, targets in valid_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Predictions\n",
    "        outputs = model(images)\n",
    "\n",
    "        for i, output in enumerate(outputs):\n",
    "            # Predicted boxes and labels\n",
    "            pred_boxes = output[\"boxes\"]\n",
    "            pred_labels = output[\"labels\"]\n",
    "\n",
    "            # Ground truth boxes and labels\n",
    "            gt_boxes = targets[i][\"boxes\"]\n",
    "            gt_labels = targets[i][\"labels\"]\n",
    "\n",
    "            # IoU matching\n",
    "            iou_matrix = box_iou(pred_boxes, gt_boxes)\n",
    "\n",
    "            matched_preds = 0\n",
    "            for pred_idx, pred_label in enumerate(pred_labels):\n",
    "                # Find the ground truth box with the highest IoU\n",
    "                max_iou, gt_idx = iou_matrix[pred_idx].max(0)\n",
    "\n",
    "                if max_iou > iou_threshold and pred_label == gt_labels[gt_idx]:\n",
    "                    matched_preds += 1\n",
    "                    iou_matrix[:, gt_idx] = 0  # Mark this GT as \"used\"\n",
    "\n",
    "            total_correct += matched_preds\n",
    "            total_ground_truth += len(gt_boxes)\n",
    "            total_images += 1\n",
    "\n",
    "    # Precision and Recall\n",
    "    precision = 100.0 * total_correct / (total_ground_truth + 1e-6)\n",
    "    recall = 100.0 * total_correct / total_images\n",
    "\n",
    "    print(f\"Precision: {precision:.2f}%\")\n",
    "    print(f\"Recall: {recall:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "Image 1:\n",
      "Predicted Boxes: tensor([[377.1245,  69.6408, 631.0519, 324.8787],\n",
      "        [315.0803,  33.4269, 513.2801, 334.7354],\n",
      "        [465.1820,  60.8906, 664.0154, 376.1479],\n",
      "        [390.5856,   3.4256, 585.0671, 252.5499],\n",
      "        [361.6699,  63.4436, 467.8331, 246.0789],\n",
      "        [392.9091,  37.2813, 500.4172, 223.9323],\n",
      "        [331.9286,  87.2681, 438.3279, 271.7551],\n",
      "        [137.9843,  40.3085, 553.5405, 383.8879],\n",
      "        [314.5021,  32.8614, 509.9504, 337.5176],\n",
      "        [228.8996,  68.5165, 479.6242, 303.5168],\n",
      "        [436.4688,   5.6550, 565.3013, 144.9261],\n",
      "        [293.9172, 106.3294, 674.0815, 298.0029],\n",
      "        [300.6774,  79.2104, 403.6188, 296.5781],\n",
      "        [361.9664,  64.5456, 466.9549, 245.4873],\n",
      "        [415.6458,  72.3263, 527.0638, 254.8941],\n",
      "        [ 82.3062,   8.6074, 551.1861, 371.3118],\n",
      "        [392.6172,   4.9982, 581.4680, 250.4277],\n",
      "        [573.8082,  54.3184, 686.4206, 287.7158],\n",
      "        [331.6018,  87.6527, 438.0057, 272.9138],\n",
      "        [297.2431,  94.2437, 502.1019, 212.4200],\n",
      "        [246.7705,  48.9961, 626.3725, 218.9636],\n",
      "        [393.6597,  39.5615, 499.2703, 221.1828],\n",
      "        [462.9882,  59.0701, 659.9094, 382.0443],\n",
      "        [347.4043,  26.0734, 451.0896, 202.1283],\n",
      "        [298.3352, 104.3543, 584.0685, 393.8519]], device='cuda:0')\n",
      "Predicted Labels: tensor([2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 3, 2,\n",
      "        2], device='cuda:0')\n",
      "Confidence Scores: tensor([0.2929, 0.2511, 0.2295, 0.2256, 0.1952, 0.1764, 0.1660, 0.1356, 0.1292,\n",
      "        0.1157, 0.1128, 0.1038, 0.0941, 0.0931, 0.0869, 0.0868, 0.0821, 0.0818,\n",
      "        0.0799, 0.0753, 0.0716, 0.0696, 0.0694, 0.0631, 0.0543],\n",
      "       device='cuda:0')\n",
      "----------\n",
      "Image 2:\n",
      "Predicted Boxes: tensor([[ 19.5175,   2.7328, 188.4683,  71.6119],\n",
      "        [ 43.8733,   0.7528, 138.7144,  51.6885],\n",
      "        [ 74.7334,   0.0000, 127.5592,  74.7088],\n",
      "        [ 38.2056,   0.0000, 144.7096, 117.8307],\n",
      "        [ 55.0872,   0.0000, 110.9851,  69.8392],\n",
      "        [ 44.1367,   1.0403, 138.2358,  50.3306],\n",
      "        [ 21.9023,   4.0778, 188.2919,  69.0238],\n",
      "        [ 74.9863,   0.0000, 126.8526,  74.3129],\n",
      "        [ 81.3892,   0.0000, 108.0220,  56.9027],\n",
      "        [ 10.9285,   0.7358, 105.6493,  53.5351],\n",
      "        [ 54.4701,   0.0000, 110.9629,  66.7516],\n",
      "        [ 38.4487,   0.0000, 143.7496, 117.8839]], device='cuda:0')\n",
      "Predicted Labels: tensor([2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3], device='cuda:0')\n",
      "Confidence Scores: tensor([0.5193, 0.4141, 0.3628, 0.2332, 0.2308, 0.1852, 0.1813, 0.1331, 0.1210,\n",
      "        0.1062, 0.0946, 0.0800], device='cuda:0')\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Test with one data\n",
    "\n",
    "test_dir = \"dataset/objectDetection/test\"\n",
    "test_annotations = \"dataset/objectDetection/test/_annotations.coco.json\"\n",
    "batch_size = 2\n",
    "test_dataset = COCODataset(train_dir, train_annotations, transforms=get_transform())\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        predictions = model(images)\n",
    "\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            print(f\"Image {i+1}:\")\n",
    "            print(\"Predicted Boxes:\", prediction[\"boxes\"])\n",
    "            print(\"Predicted Labels:\", prediction[\"labels\"])\n",
    "            print(\"Confidence Scores:\", prediction[\"scores\"])\n",
    "            print(\"----------\")\n",
    "        break  # Test on only the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved Successfully\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "\n",
    "torch.save(model.state_dict(), \"model/v1.pth\")\n",
    "print(\"Model Saved Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lenovo\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_5164\\326388462.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model/v1.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "\n",
    "num_classes = 4  # 3 pressure ulcer stages + 1 background\n",
    "model = get_model(num_classes)\n",
    "model.load_state_dict(torch.load(\"model/v1.pth\"))\n",
    "print(\"Model Loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train\\045069bee69b9a6b457a69c564589895_0_png.rf.60f1b96146bdbb6713259dec4a7b224a.jpg\n"
     ]
    }
   ],
   "source": [
    "a = \"dataset/train\"\n",
    "b = \"045069bee69b9a6b457a69c564589895_0_png.rf.60f1b96146bdbb6713259dec4a7b224a.jpg\"\n",
    "image_path = os.path.join(a, b)\n",
    "print(image_path)\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
